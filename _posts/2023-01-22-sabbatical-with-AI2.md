---
layout: post
title: I'm on sabbatical 
subtitle: I'm taking my first ever sabbatical after 15 years as a faculty member to do research with AI2.
cover-img:
thumbnail-img:
share-img:
tags: [sabbatical, research]
---

I'm taking my first ever sabbatical after 15 years as a faculty member.  I'm going to be doing research with AI2, the Allen Institute for Artificial Intelligence.  AI2 is a non-profit research institute funded by Paul G. Allen, the co-founder of Microsoft.  AI2 is based in Seattle, but due to family constraints I'll mainly be mainly working remotely from Philadelphia.  I hope to spend a good chunk of the summer in Seattle in person. 

AI2 is home to a lot of fabulous researchers and really exciting projects!  I'll be working with the [Aristo team](https://allenai.org/aristo) lead by [Peter Clark](https://allenai.org/team/peterc).

My goals for my sabbatical are:
* Decide on a new research agenda for the next few years.  I'm trying to work through what contributions I can make to the field in the era of LLMs.  I have a hunch that LLMs will let us re-visit some classic AI approaches to automated symbolic reasoning.  A talk that Peter Clark gave at Penn about AI2's EntailmentWriter system got me excited to work with the Aristo team, since I think they are ahead of the curve on the contributions to neuro-symbolic reasoning.
* Collaborate with the AI2 researchers on their ongoing project and on some of mine. I'm also hoping to include my PhD students in our joint collaborations.  I would love for my PhD students to get exposed to the research at AI2 and to have a chance to be mentored by researchers there. 


A few of the research topics that I pitched to the Aristo team are:
1. Text adventure games + LLMs
2. LLMs to enable symbolic reasoners
3. Intelligent Tutors
4. AI + Human Interaction to build structured representations that can be reasoned over
5. Explainable models, especially via concept bottleneck models generated by LLMs.

## Text adventure games + LLMs

Last year, I co-taught a course on [Interactive Fiction and Text Generation](https://interactive-fiction-class.org) with my postdoc [Lara Martin](https://laramartin.net).  I really like the idea that language based games can be a good test bed for AI in the era of LLMs, in the same way that Chess or Go or Atari were good games to test previous approaches to AI.

In a project that I did at Google as a part time visiting researcher, we tried to teach their [LaMDA system to play Dungeons and Dragons](https://www.cis.upenn.edu/~ccb/publications/dungeons-and-dragons-as-a-dialog-challenge-for-artificial-intelligence.pdf).   I also just finished a [follow-on paper](https://arxiv.org/abs/2212.10060) with several researchers at AI2, including Pei Zhou,  Xiang Ren, Yejin Choi and Raj Ammanabrolu, which looked at trying to guide the game conversation using Theory of Mind.

I'm also really excited by research that Raj Ammanabrolu did with frequent AI2 collaborator Peter Jansen and others on [ScienceWorld](https://arxiv.org/abs/2203.07540).  ScienceWorld is a interactive text environment that simulates science and tests the AI agents abilities to reason about science concepts like conductivity. I think that simulations are a very cool way to explore the capabilities of LLMs.

Finally, I think that having a closed-world text adventure game offers some very cool potentials for anchoring LLMs in a grounded world-state representation.  Last weekend, I was playing with a class assignment that I wrote for my interactive fiction class, and having GPT3 narrate the game given a textualized game state.

## LLMs to enable symbolic reasoners

One of the things that I think are exciting about LLMs is that they potentially allow us to translate natural language into a symbolic representation.  Then we can use that symbolic representation for performing automated reasoning using old-school techniques like classical planning.

One project that I'd like to attempt is to convert NL onto a classic representation called PDDL (Planning Domain Definition Language).   PDDL uses an action schema to represent actions. Here is an example of an action schema for flying a plane from one location to another:
```
(:action fly
     :parameters (?p - plane ?from - airport ?to - airport)
     :precondition (and (plane ?p) (airport ?from) (airport ?to) (at ?p ?from))
     :effect (and (at ?p ?to)) (not (at ?p ?from)))
)
```
This defines an action called fly, which takes 3 arguments: a plane p, a starting airport from and a destination airport to. In order for this action to be applied, several preonditions must be satisified:
1. p must be a plane
2. from must be be an airport
3. to must be be an airport
4. p must initially be located at from.

In my class interactive fiction class, I had students use PDDL and classical planning to solve text adventure games.  I also had them [translate WikiHow articles into PDDL](https://interactive-fiction-class.org/homeworks/planning/planning.html) in the hopes that we could use that as training data to automate the process of generating action schema from text, and then applying symbolic reasoners.

## Intelligent Tutors

Because I've been teaching classes with >500 students, I've started thinking through how we could automated different parts of teaching, and how my students can be used to improve our AI systems.

My students and I recently published [a small study at ACL on automatically generating quiz questions](https://www.cis.upenn.edu/~ccb/publications/smart-textbook-feasibility-study.pdf) that found using  summarization helps generate higher quality quiz questions.

Subsequent to that study, I have collected several hundred student written summaries of the Russell and Norvig textbook "Artificial Intelligence: A Modern Approach", and the Jurafsky and Marin textbook "Speech and Language Processing".   We've been using those summaries to generate more quiz questions, to do concept (bold text prediction), and to expand bulleted outlines into full sentences.

I've got a bunch of great undergrads working on this project, including a current Penn student named Hannah Gonzalez, and two former Penn students Shriyash Upadhyay and Etan Ginsberg who have created an Ed Tech startup called [Martian](https://learn.withmartian.com/).

I'm also very interested in how we can use tools like ChatGPT to improve education.  Right now, I feel that the fear is that it'll ruin education by making plagiarism too hard to spot (although some of my students have done [research on measuring human abilities to detect machine v. human-generated text](https://www.cis.upenn.edu/~ccb/publications/real-or-fake-text-analysis.pdf) and on [automatic algorithms for spotting computer generated text](https://www.cis.upenn.edu/~ccb/publications/automatic-detection-of-generated-text-is-easiest-when-humans-are-fooled.pdf)). I'd love to get educators thinking about how they could make effective use of tools like ChatGPT, rather than simply be worried about them.

## AI + Human Interaction to build structured representations that can be reasoned over

Similar to the idea that LLMs could be used to generate PDDL representations, I'd love to build tools to interactively construct other structured representations that can be reasoned over.  I'm thinking things like
* PDDL
* Timelines
* Logical Rules
* Schema
* BayesNets

Here's an example of a student project that I supervised last semester using GPT3 to generate event schema for a DARPA project called KAIROS: [https://dev.kairos.jiaxuan.me](https://dev.kairos.jiaxuan.me)

## Explainable models, especially via concept bottleneck models generated by LLMs

I'm also very excited by explainable models.  Some of my PhD students have been working on variants of Concept Bottleneck Models, which were originally proposed by Percy Liang's group at Stanford.  Rather than using a neural net to do full end-to-end image classification, they introduced a set of human written "concepts" (for bird classification these would be features like beak shape or wing tip color), and then had the NN predict these concepts.  Then a simple linear classifier was used on these features.  The idea being the linear classifier is more interpretable.  We tried a variant of this idea where we used a LLM to generate the concept.  Here's our [ArXiv draft](https://arxiv.org/abs/2211.11158).


